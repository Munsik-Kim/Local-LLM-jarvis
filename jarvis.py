import ollama
import sys
import os
from tts_engine import speak 
from vision_engine import analyze_image
from function_engine import execute_command, save_to_file # save_to_file ì¶”ê°€ë¨

messages = [
    {
        'role': 'system', 
        'content': """
        ë„ˆëŠ” ìœ ëŠ¥í•œ ë¹„ì„œ 'ìë¹„ìŠ¤'ë‹¤.
        1. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ëŒ€ë‹µí•´ë¼.
        2. í•œìëŠ” ì“°ì§€ ë§ˆë¼.
        3. ë§Œì•½ ì‚¬ìš©ìê°€ ì œê³µí•œ 'ê²€ìƒ‰ ê²°ê³¼'ê°€ ìˆë‹¤ë©´, ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì•„ì£¼ ìƒì„¸í•˜ê²Œ ìš”ì•½í•´ë¼.
        """
    }
]

print("==================================================")
print("ğŸ¦¾ ìë¹„ìŠ¤(Jarvis) ì—ì´ì „íŠ¸ ëª¨ë“œ ê°€ë™")
print("==================================================")
speak("ì—…ê·¸ë ˆì´ë“œ ì™„ë£Œ. ì¸í„°ë„· ì¡°ì‚¬ ë° ë³´ê³ ì„œ ì‘ì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

while True:
    try:
        user_input = input("\nğŸ‘¤ ì‚¬ìš©ì: ")
        clean_input = user_input.strip()
        
        if clean_input.lower() in ["exit", "ì¢…ë£Œ"]:
            break
        if not clean_input:
            continue

        # 1. ëª…ë ¹ì–´ ì‹¤í–‰ í™•ì¸
        is_command, result_msg = execute_command(clean_input)
        
        if is_command:
            # ------------------------------------------------------------
            # â­ í•µì‹¬ ë¡œì§: ê²€ìƒ‰ ê²°ê³¼ê°€ ëŒì•„ì™”ì„ ë•Œ (REPORT: í‘œì‹ í™•ì¸)
            # ------------------------------------------------------------
            if result_msg.startswith("REPORT:"):
                # "REPORT:í‚¤ì›Œë“œ" ì™€ "ë‚´ìš©"ì„ ë¶„ë¦¬
                lines = result_msg.split("\n", 1)
                keyword = lines[0].replace("REPORT:", "")
                search_data = lines[1]
                
                print(f"ğŸ§  ìë¹„ìŠ¤: ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ì½ê³  ë³´ê³ ì„œë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤... (í‚¤ì›Œë“œ: {keyword})")
                speak(f"{keyword}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ë‚´ìš©ì„ ìš”ì•½í•´ ë“œë¦´ê²Œìš”.")
                
                # LLMì—ê²Œ ê²€ìƒ‰ ë‚´ìš© ë˜ì ¸ì£¼ê³  ìš”ì•½ì‹œí‚¤ê¸°
                prompt = f"""
                ë‹¤ìŒì€ ì¸í„°ë„·ì—ì„œ ê²€ìƒ‰ëœ '{keyword}' ê´€ë ¨ ì •ë³´ë‹¤.
                ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜.
                ì„œë¡ , ë³¸ë¡ , ê²°ë¡ ìœ¼ë¡œ ë‚˜ëˆ„ê³  í•µì‹¬ ë‚´ìš©ì„ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¡œ ì •ë¦¬í•´.
                
                [ê²€ìƒ‰ ë°ì´í„°]
                {search_data}
                """
                
                # LLM ìƒì„± ì‹œì‘
                stream = ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': prompt}], stream=True)
                
                full_report = ""
                print(f"\nğŸ“„ [{keyword} ë³´ê³ ì„œ ìƒì„± ì¤‘...]\n")
                for chunk in stream:
                    part = chunk['message']['content']
                    print(part, end='', flush=True)
                    full_report += part
                print("\n")
                
                # â­ íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°
                saved_path = save_to_file(keyword, full_report)
                print(f"ğŸ’¾ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {saved_path}")
                speak("ë³´ê³ ì„œ ì‘ì„±ì„ ì™„ë£Œí•˜ê³  íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                
                # ê¸°ì–µì— ì¶”ê°€
                messages.append({'role': 'assistant', 'content': f"{keyword} ì¡°ì‚¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤."})
                continue

            # ì¼ë°˜ ëª…ë ¹ì–´(ë©”ëª¨ì¥, ìœ íŠœë¸Œ ë“±)ì¸ ê²½ìš°
            else:
                print(f"ğŸ¤– ìë¹„ìŠ¤: {result_msg}")
                speak(result_msg)
                continue

        # 2. ì´ë¯¸ì§€ ì²˜ë¦¬
        if os.path.isfile(clean_input):
            # ... (ê¸°ì¡´ ì´ë¯¸ì§€ ì²˜ë¦¬ ì½”ë“œ) ...
            pass 
            
        # 3. ì¼ë°˜ ëŒ€í™”
        messages.append({'role': 'user', 'content': clean_input})
        # ... (ê¸°ì¡´ ëŒ€í™” ì½”ë“œ) ...
        # ì—¬ê¸°ì„œëŠ” ìƒëµí–ˆì§€ë§Œ ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ ë©ë‹ˆë‹¤.
        
        # (í¸ì˜ë¥¼ ìœ„í•´ ì¼ë°˜ ëŒ€í™” ë¶€ë¶„ ì½”ë“œë¥¼ ê°„ëµíˆ ì ìŠµë‹ˆë‹¤. ì‹¤ì œ íŒŒì¼ì—” ê¸°ì¡´ ë‚´ìš©ì„ ìœ ì§€í•˜ì„¸ìš”)
        print("ğŸ¤– ìë¹„ìŠ¤: ", end="")
        stream = ollama.chat(model='llama3.1', messages=messages, stream=True)
        full_res = ""
        for chunk in stream:
            part = chunk['message']['content']
            print(part, end="", flush=True)
            full_res += part
        print()
        speak(full_res)
        messages.append({'role': 'assistant', 'content': full_res})

    except KeyboardInterrupt:
        break